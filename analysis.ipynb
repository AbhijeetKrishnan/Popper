{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = 'tactics/data/stats/metrics_test_t1600_m1600.csv'\n",
    "df = pd.read_csv(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby('tactic_text')\n",
    "df_grouped.describe()\n",
    "agg = df_grouped.aggregate(np.nansum).reset_index()\n",
    "mask = agg['tactic_text'].isin(['ground', 'engine_best'])\n",
    "agg_masked = agg[~mask]\n",
    "agg_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg['avg_tactic_ground_div'] = agg['tactic_ground_div'] / agg['match']\n",
    "mask = agg['tactic_text'].isin(['ground', 'engine_best'])\n",
    "agg_masked = agg[~mask]\n",
    "plt.hist(agg_masked['avg_tactic_ground_div'], bins=10)\n",
    "plt.axvline(agg[agg['tactic_text'] == \"f(A,B,C):-legal_move(B,C,A)\"]['avg_tactic_ground_div'].values, linestyle='dashed', color='blue', label='random move tactic')\n",
    "plt.axvline(agg[agg['tactic_text'] == \"ground\"]['avg_tactic_ground_div'].values, linestyle='dashed', color='green', label='ground move tactic')\n",
    "plt.axvline(agg[agg['tactic_text'] == \"engine_best\"]['avg_tactic_ground_div'].values, linestyle='dashed', color='red', label='engine move tactic')\n",
    "plt.title('Histogram of Divergence from Ground $(T,$ Maia-1600, $P_{valid})$')\n",
    "plt.xlabel('Divergence (Cp)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(left=-10)\n",
    "\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg['coverage'] = agg['match'] / df.groupby(['position', 'move']).ngroups\n",
    "mask = agg['tactic_text'].isin(['ground', 'engine_best'])\n",
    "agg_masked = agg[~mask]\n",
    "plt.hist(agg_masked['coverage'], bins=10)\n",
    "plt.axvline(agg[agg['tactic_text'] == \"f(A,B,C):-legal_move(B,C,A)\"]['coverage'].values, linestyle='dashed', color='blue', label='random move tactic')\n",
    "plt.axvline(agg[agg['tactic_text'] == \"ground\"]['coverage'].values, linestyle='dashed', color='green', label='ground move tactic')\n",
    "plt.axvline(agg[agg['tactic_text'] == \"engine_best\"]['coverage'].values, linestyle='dashed', color='red', label='engine move tactic')\n",
    "plt.title('Histogram of Coverage $(T$, $P_{valid})$')\n",
    "plt.xlabel('Coverage')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(left=0)\n",
    "\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg['accuracy'] = agg['correct_move'] / agg['match']\n",
    "mask = agg['tactic_text'].isin(['ground', 'engine_best'])\n",
    "agg_masked = agg[~mask]\n",
    "plt.hist(agg_masked['accuracy'], bins=10)\n",
    "plt.axvline(agg[agg['tactic_text'] == \"f(A,B,C):-legal_move(B,C,A)\"]['accuracy'].values, linestyle='dashed', color='blue', label='random move tactic')\n",
    "plt.axvline(agg[agg['tactic_text'] == \"ground\"]['accuracy'].values, linestyle='dashed', color='green', label='ground move tactic')\n",
    "plt.axvline(agg[agg['tactic_text'] == \"engine_best\"]['accuracy'].values, linestyle='dashed', color='red', label='engine move tactic')\n",
    "plt.title('Histogram of Accuracy $(T$, $P_{valid})$')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(right=None)\n",
    "\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = agg.sort_values(by = ['avg_tactic_ground_div'], ascending = [True])\n",
    "print(list(final['tactic_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = agg['tactic_text'].isin(['ground', 'engine_best'])\n",
    "agg_masked = agg[~mask]\n",
    "avg_div = agg_masked['avg_tactic_ground_div'].mean()\n",
    "random_div = agg[agg['tactic_text'] == \"f(A,B,C):-legal_move(B,C,A)\"]['avg_tactic_ground_div'].values\n",
    "(random_div - avg_div) / random_div * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Position Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_move'] = df['position'] + \",\" + df['move']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby('pos_move')\n",
    "agg = df2.aggregate(np.sum)\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(agg['exec_time'], bins=100)\n",
    "plt.title('Histogram of Execution Time $(T_{1600},$ Stockfish 14, $P_{valid})$')\n",
    "plt.xlabel('Execution Time (s)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_exec = agg[agg['exec_time'] > 100]\n",
    "high_exec = high_exec.sort_values(by = ['exec_time'], ascending = [False])\n",
    "high_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_exec = agg[agg['exec_time'] <= 100]\n",
    "plt.hist(low_exec['exec_time'], bins=100)\n",
    "plt.title('Histogram of Execution Time $(T_{1600},$ Stockfish 14, $P_{valid})$')\n",
    "plt.xlabel('Execution Time (s)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_exs = [1, 5, 20, 100]\n",
    "time_vals = [27.5, 60 + 33, 60 * 6 + 33, 33 * 60 + 8]\n",
    "num_tactics = [9, 117, 469, 837]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_exs, time_vals)\n",
    "plt.xlabel('Number of train examples')\n",
    "plt.ylabel('Execution time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_exs, num_tactics)\n",
    "plt.xlabel('Number of train examples')\n",
    "plt.ylabel('Number of tactics learned')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_tactics, time_vals)\n",
    "plt.xlabel('Number of tactics learned')\n",
    "plt.ylabel('Execution time (s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_games = 0\n",
    "valid_elo = 0\n",
    "total_pos = 0\n",
    "total_elo = 0\n",
    "game_count = Counter()\n",
    "\n",
    "pgn_path = 'tactics/data/lichess_db_standard_rated_2013-01.pgn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = []\n",
    "handle = open(pgn_path)\n",
    "header = chess.pgn.read_headers(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header.get('Termination')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "\n",
    "with open(pgn_path) as pgn:\n",
    "    while game := chess.pgn.read_game(pgn):\n",
    "        total_games += 1\n",
    "        white_elo = game.headers['WhiteElo']\n",
    "        black_elo = game.headers['BlackElo']\n",
    "        if '?' not in white_elo and '?' not in black_elo:\n",
    "            total_elo += int(white_elo)\n",
    "            total_elo += int(black_elo)\n",
    "            valid_elo += 1\n",
    "        game_count[game.headers['Event']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total games', total_games)\n",
    "avg_elo = total_elo / (2 * valid_elo)\n",
    "print('Average ELO', avg_elo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = 0\n",
    "\n",
    "with open(pgn_path) as pgn:\n",
    "    while game := chess.pgn.read_game(pgn):\n",
    "        total_games += 1\n",
    "        white_elo = game.headers['WhiteElo']\n",
    "        black_elo = game.headers['BlackElo']\n",
    "        if '?' not in white_elo and '?' not in black_elo:\n",
    "            variance += (int(white_elo) - avg_elo) ** 2\n",
    "            variance += (int(black_elo) - avg_elo) ** 2\n",
    "        game_count[game.headers['Event']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "print('SD', math.sqrt(variance / valid_elo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "from tactics.util import *\n",
    "\n",
    "board = chess.Board('r1bqkb1r/ppp1pnpp/3p4/7Q/8/3B4/PPP2PPP/RNB2RK1 w kq - 0 10')\n",
    "move_names = [\n",
    "    'b1c3',\n",
    "    'c1g5',\n",
    "    'd3c4',\n",
    "    'f1e1'\n",
    "]\n",
    "moves = [chess.Move.from_uci(move_name) for move_name in move_names]\n",
    "engine_path = get_lc0_cmd(LC0, MAIA_1600) + ['--nncache=0']\n",
    "#engine_path = STOCKFISH\n",
    "mate_score = 2000\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with chess.engine.SimpleEngine.popen_uci(engine_path) as engine:\n",
    "    tmp_analysis = engine.analyse(board, limit=chess.engine.Limit(nodes=1), multipv=n, game=object())\n",
    "    top_results = [(root['pv'][0], root['score'].relative.score(mate_score=mate_score)) for root in tmp_analysis]\n",
    "    top_n_results = top_results[:n]\n",
    "top_n_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with chess.engine.SimpleEngine.popen_uci(engine_path) as engine:\n",
    "    all_evals = get_evals(engine, board, board.legal_moves)\n",
    "    all_evals.sort(key=lambda eval: eval[1], reverse=True)\n",
    "    top_n_results = all_evals[:n]\n",
    "top_n_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evals(engine: chess.engine.SimpleEngine, board: chess.Board, suggestions: List[chess.Move], mate_score: int=2000) -> List[Tuple[chess.Move, int]]:\n",
    "    \"Obtain engine evaluations for a list of moves in a given position\"\n",
    "\n",
    "    evals = []\n",
    "    for move in suggestions:\n",
    "        tmp_board = chess.Board(board.fen())\n",
    "        tmp_board.push(move)\n",
    "        if tmp_board.outcome() is not None:\n",
    "            move_score = mate_score if tmp_board.is_checkmate() else -mate_score\n",
    "            evals.append((move, move_score))\n",
    "            continue\n",
    "        limit = chess.engine.Limit(nodes=1)\n",
    "        # prev_eval = engine.analyse(board, limit=limit, game=object()) # https://stackoverflow.com/a/66251120\n",
    "        # print(prev_eval) \n",
    "        curr_eval = engine.analyse(tmp_board, limit=limit, game=object())\n",
    "        print(curr_eval)\n",
    "        orig_turn = board.turn\n",
    "        if 'pv' in curr_eval:\n",
    "            # prev_score = prev_eval['score'].pov(orig_turn)\n",
    "            curr_score = curr_eval['score'].pov(orig_turn)\n",
    "            move_score = curr_score.score(mate_score=mate_score)\n",
    "            evals.append((move, move_score))\n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with chess.engine.SimpleEngine.popen_uci(engine_path) as engine:\n",
    "    evals = get_evals(engine, board, moves)\n",
    "evals"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6355374956c5778604016e244f979cc331398a530384f2842e5bccfbc3abef60"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('tactics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
