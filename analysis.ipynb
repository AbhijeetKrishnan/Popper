{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = 'tactics/data/stats/metrics_valid_maia1600.csv'\n",
    "df = pd.read_csv(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby('tactic_text')\n",
    "df2.describe()\n",
    "agg = df2.aggregate(np.sum)\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg['avg_divergence'] = agg['divergence'] / agg['matches']\n",
    "plt.hist(agg['avg_divergence'], bins=10)agg.loc[[\"f(A,B,C):-legal_move(B,C,A)\"]]['avg_divergence'].values\n",
    "plt.axvline(, linestyle='dashed')\n",
    "plt.title('Histogram of Divergence $(T_{1600},$ Stockfish 14, $P_{test})$')\n",
    "plt.xlabel('Divergence (Cp)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['text'] == \"f(A,B,C):-legal_move(B,C,A)\")]['avg_divergence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['avg'], bins=10)\n",
    "plt.axvline(df.loc[(df['text'] == \"f(A,B,C):-legal_move(B,C,A)\")]['avg_divergence'].values, linestyle='dashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg['coverage'] = agg['matches'] / df.groupby(['position', 'move']).ngroups\n",
    "plt.hist(agg['coverage'], bins=10)\n",
    "plt.axvline(agg.loc[[\"f(A,B,C):-legal_move(B,C,A)\"]]['coverage'].values, linestyle='dashed')\n",
    "plt.title('Histogram of Coverage $(T_{1600}$, $P_{test})$')\n",
    "plt.xlabel('Coverage')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg['accuracy'] = agg['correct_move'] / agg['matches']\n",
    "plt.hist(agg['accuracy'], bins=10)\n",
    "plt.axvline(agg.loc[[\"f(A,B,C):-legal_move(B,C,A)\"]]['accuracy'].values, linestyle='dashed')\n",
    "plt.title('Histogram of Accuracy $(T_{1600}$, $P_{test})$')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(agg['tactic_evals'], bins=10)\n",
    "plt.axvline(agg.loc[[\"f(A,B,C):-legal_move(B,C,A)\"]]['tactic_evals'].values, linestyle='dashed', color='blue', label='Random tactic evaluation')\n",
    "plt.axvline(agg.loc[[\"f(A,B,C):-legal_move(B,C,A)\"]]['ground_evals'].values, linestyle='dashed', color='green', label='Ground move evaluation')\n",
    "plt.axvline(agg.loc[[\"f(A,B,C):-legal_move(B,C,A)\"]]['best_move_evals'].values, linestyle='dashed', color='red', label='Stockfish 14 best move evaluation')\n",
    "plt.xlabel('Total Evaluation Score (Cp)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(loc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = agg.sort_values(by = ['avg_divergence'], ascending = [True])\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(final.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_games = 0\n",
    "valid_elo = 0\n",
    "total_pos = 0\n",
    "total_elo = 0\n",
    "game_count = Counter()\n",
    "\n",
    "pgn_path = 'tactics/data/lichess_db_standard_rated_2013-01.pgn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = []\n",
    "handle = open(pgn_path)\n",
    "header = chess.pgn.read_headers(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header.get('Termination')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess.pgn\n",
    "\n",
    "with open(pgn_path) as pgn:\n",
    "    while game := chess.pgn.read_game(pgn):\n",
    "        total_games += 1\n",
    "        white_elo = game.headers['WhiteElo']\n",
    "        black_elo = game.headers['BlackElo']\n",
    "        if '?' not in white_elo and '?' not in black_elo:\n",
    "            total_elo += int(white_elo)\n",
    "            total_elo += int(black_elo)\n",
    "            valid_elo += 1\n",
    "        game_count[game.headers['Event']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total games', total_games)\n",
    "avg_elo = total_elo / (2 * valid_elo)\n",
    "print('Average ELO', avg_elo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = 0\n",
    "\n",
    "with open(pgn_path) as pgn:\n",
    "    while game := chess.pgn.read_game(pgn):\n",
    "        total_games += 1\n",
    "        white_elo = game.headers['WhiteElo']\n",
    "        black_elo = game.headers['BlackElo']\n",
    "        if '?' not in white_elo and '?' not in black_elo:\n",
    "            variance += (int(white_elo) - avg_elo) ** 2\n",
    "            variance += (int(black_elo) - avg_elo) ** 2\n",
    "        game_count[game.headers['Event']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "print('SD', math.sqrt(variance / valid_elo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "from tactics.util import *\n",
    "\n",
    "board = chess.Board('r1bqk1nr/ppp2ppp/3b4/3p4/8/2PNP3/PP3PPP/RNBQKB1R b KQkq - 2 7')\n",
    "move1 = chess.Move.from_uci('g8h6')\n",
    "move2 = chess.Move.from_uci('g8f6')\n",
    "engine_path = get_lc0_cmd(LC0, MAIA_1900) + ['--verbose-move-stats']\n",
    "#engine_path = STOCKFISH\n",
    "mate_score = 2000\n",
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with chess.engine.SimpleEngine.popen_uci(engine_path) as engine:\n",
    "    analysis = engine.analyse(board, limit=chess.engine.Limit(depth=1), multipv=n, game=object())\n",
    "    top_results = [(root['pv'][0], root['score'].relative.score(mate_score=mate_score)) for root in analysis]\n",
    "    top_n_results = top_results[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with chess.engine.SimpleEngine.popen_uci(engine_path) as engine:\n",
    "    evals = get_evals(engine, board, [move1, move2])\n",
    "evals"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6355374956c5778604016e244f979cc331398a530384f2842e5bccfbc3abef60"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('tactics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
